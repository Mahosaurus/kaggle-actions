{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from rwanda.src.utils import get_repo_root\n",
    "from rwanda.src.impute import impute_data\n",
    "from rwanda.src.reduce import reduce_dimenions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "train = pd.read_csv(f'{get_repo_root()}/data/train.csv')\n",
    "# Read test data\n",
    "test = pd.read_csv(f'{get_repo_root()}/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make ID and datevar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From data frame train, extract from ID_LAT_LON_YEAR_WEEK column ID_LAT_LON as a new column\n",
    "train['ID_LAT_LON'] = [\"_\".join(x.split('_')[0:3]) for x in train['ID_LAT_LON_YEAR_WEEK']]\n",
    "test['ID_LAT_LON'] = [\"_\".join(x.split('_')[0:3]) for x in test['ID_LAT_LON_YEAR_WEEK']]\n",
    "# From data frame train, extract from ID_LAT_LON_YEAR_WEEK column YEAR_WEEK as a new column\n",
    "train[\"year_week\"] = [\"_\".join(x.split('_')[3:5]) for x in train['ID_LAT_LON_YEAR_WEEK']]\n",
    "test[\"year_week\"] = [\"_\".join(x.split('_')[3:5]) for x in test['ID_LAT_LON_YEAR_WEEK']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = impute_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dim Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = reduce_dimenions(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummify ID_LAT_LON\n",
    "train = pd.get_dummies(train, columns=['ID_LAT_LON'])\n",
    "test = pd.get_dummies(test, columns=['ID_LAT_LON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all presumably useless vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all env vars\n",
    "train = train.drop(columns=[col for col in train.columns if 'Sulphur' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Nitrogen' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Ozone' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Carbon' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Uv' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Cloud' in col])\n",
    "train = train.drop(columns=[col for col in train.columns if 'Form' in col])\n",
    "\n",
    "test = test.drop(columns=[col for col in test.columns if 'Sulphur' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Nitrogen' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Ozone' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Carbon' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Uv' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Cloud' in col])\n",
    "test = test.drop(columns=[col for col in test.columns if 'Form' in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify outlier value for ID_LAT_LON_ID_-2.378_29.222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 1100 to train['ID_LAT_LON_YEAR_WEEK'] where ID_LAT_LON_YEAR_WEEK==ID_LAT_LON_ID_-2.378_29.222_2021_09\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_09', \"emission\"] = 1100\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_00', \"emission\"] = 2700\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_01', \"emission\"] = 2800\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_02', \"emission\"] = 2900\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_03', \"emission\"] = 2900\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_04', \"emission\"] = 3000\n",
    "train.loc[train['ID_LAT_LON_YEAR_WEEK'] == 'ID_-2.079_29.321_2021_05', \"emission\"] = 2900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add feature: ID_LAT_LON_ID_-2.079_29.321 * week_no\n",
    "\n",
    "because this ID has large values and an easy pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interaction feature: ID_LAT_LON_ID_-2.079_29.321 * week_no\n",
    "# Add 1 otherwise the interaction feature will be 1 for week 0 for all observations\n",
    "train['week_no'] = train['week_no'] + 1\n",
    "test['week_no'] = test['week_no'] + 1\n",
    "train['ID_LAT_LON_ID_-2.079_29.321_week_no'] = train['ID_LAT_LON_ID_-2.079_29.321'] * train['week_no']\n",
    "test['ID_LAT_LON_ID_-2.079_29.321_week_no'] = test['ID_LAT_LON_ID_-2.079_29.321'] * test['week_no']\n",
    "# Dummify\n",
    "train = pd.get_dummies(train, columns=['ID_LAT_LON_ID_-2.079_29.321_week_no'])\n",
    "test = pd.get_dummies(test, columns=['ID_LAT_LON_ID_-2.079_29.321_week_no'])\n",
    "# Add missing columns\n",
    "test['ID_LAT_LON_ID_-2.079_29.321_week_no_50'] = 0\n",
    "test['ID_LAT_LON_ID_-2.079_29.321_week_no_51'] = 0\n",
    "test['ID_LAT_LON_ID_-2.079_29.321_week_no_52'] = 0\n",
    "test['ID_LAT_LON_ID_-2.079_29.321_week_no_53'] = 0\n",
    "\n",
    "# Add interaction feature: ID_LAT_LON_ID_-2.378_29.222 * week_no\n",
    "train['ID_LAT_LON_ID_-2.378_29.222_week_no'] = train['ID_LAT_LON_ID_-2.378_29.222'] * train['week_no']\n",
    "test['ID_LAT_LON_ID_-2.378_29.222_week_no'] = test['ID_LAT_LON_ID_-2.378_29.222'] * test['week_no']\n",
    "# Dummify\n",
    "train = pd.get_dummies(train, columns=['ID_LAT_LON_ID_-2.378_29.222_week_no'])\n",
    "test = pd.get_dummies(test, columns=['ID_LAT_LON_ID_-2.378_29.222_week_no'])\n",
    "# Add missing columns\n",
    "test['ID_LAT_LON_ID_-2.378_29.222_week_no_50'] = 0\n",
    "test['ID_LAT_LON_ID_-2.378_29.222_week_no_51'] = 0\n",
    "test['ID_LAT_LON_ID_-2.378_29.222_week_no_52'] = 0\n",
    "test['ID_LAT_LON_ID_-2.378_29.222_week_no_53'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the weight of ID_LAT_LON_ID_-2.079_29.321 & ID_LAT_LON_ID_-1.514_29.686 by duplicating the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = train[train['ID_LAT_LON_ID_-2.079_29.321'] == 1]\n",
    "duplicate[\"emission\"] = duplicate[\"emission\"].apply(lambda x: x + 1)\n",
    "train = pd.concat([train, duplicate], ignore_index=True)\n",
    "\n",
    "duplicate = train[train['ID_LAT_LON_ID_-1.514_29.686'] == 1]\n",
    "duplicate[\"emission\"] = duplicate[\"emission\"].apply(lambda x: x + 1)\n",
    "train = pd.concat([train, duplicate], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 2020 between week 10 and 38 from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove week_no between x and y of 2020 from dataset\n",
    "train = train[~((train['year'] == 2020) & (train['week_no'] >= 10) & (train['week_no'] <= 40))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model on train dataframe, X is all features\n",
    "# but 'ID_LAT_LON_YEAR_WEEK', 'latitude', 'longitude', 'year' and 'emission'\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, TweedieRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "features = [col for col in train.columns if col not in ['ID_LAT_LON_YEAR_WEEK', 'latitude', 'longitude', 'year', 'emission']]\n",
    "\n",
    "# Determine X and y\n",
    "X = train[features]\n",
    "y = train['emission']\n",
    "\n",
    "# Train, Validation Split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a basic linear regression model on X_train and y_train\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model on X_val and y_val\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred[y_pred < 0] = 0\n",
    "print('MSE: ', mean_squared_error(y_val, y_pred))\n",
    "print('R2: ', r2_score(y_val, y_pred))\n",
    "\n",
    "# Append predictions to X_val dataframe\n",
    "X_val['pred'] = y_pred\n",
    "X_val['y_val'] = y_val\n",
    "X_val['abs_error'] = abs(X_val['pred'] - X_val['y_val'])\n",
    "# Show top 10 errors\n",
    "top_errors = X_val.sort_values(by='abs_error', ascending=False).head(10)\n",
    "# Drop all columns where there are only 0s\n",
    "top_errors.loc[:, (top_errors != 0).any(axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "train[\"pred\"] = y_pred\n",
    "train[\"abs_error\"] = abs(train[\"emission\"] - train[\"pred\"])\n",
    "# Make emission to 0 when it is negative\n",
    "y_pred[y_pred < 0] = 0\n",
    "print('MSE: ', mean_squared_error(y, y_pred))\n",
    "print('R2: ',  r2_score(y, y_pred))\n",
    "print(\"MSE: \", mean_squared_error(train[\"emission\"], train[\"pred\"]) )\n",
    "\n",
    "# Show top 10 errors\n",
    "top_errors = train.sort_values(by='abs_error', ascending=False).head(5)\n",
    "# Drop all columns where there are only 0s\n",
    "top_errors.loc[:, (top_errors != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make submission\n",
    "X_test = test[features]\n",
    "test[\"emission\"] = model.predict(X_test)\n",
    "# Make emission to 0 when it is negative\n",
    "test.loc[test[\"emission\"] < 0, \"emission\"] = 0\n",
    "test = test[[\"ID_LAT_LON_YEAR_WEEK\", \"emission\"]]\n",
    "test.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwanda-qMl64OKB-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
